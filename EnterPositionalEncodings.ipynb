{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPI6OxK9rH6NUvdNRjePFhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inventwithdean/transformers/blob/main/EnterPositionalEncodings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZk8oH5j9I-f"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "Keys = torch.randn((6, 6))\n",
        "Queries = torch.randn((6, 6))\n",
        "Values = torch.randn((6, 6))\n",
        "EmbeddingMat = torch.randn((3, 6))\n",
        "a = torch.tensor(([0, 1, 2]))"
      ],
      "metadata": {
        "id": "DrjbtiXX9NOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = EmbeddingMat[a] # Embeddings of three random words\n",
        "a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGdLHNey9Ver",
        "outputId": "8c20812d-cb0d-4725-c85b-78049814fa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.6218,  1.0775,  0.4941,  0.4672, -0.3454, -1.1625],\n",
              "        [ 0.1445,  0.1663,  0.7507,  0.9132, -1.4355, -1.5304],\n",
              "        [ 0.9593,  1.0600,  0.6299, -1.2867, -0.6875,  2.1382]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = a @ Keys\n",
        "q = a @ Queries\n",
        "v = a @ Values\n",
        "attention = q @ k.T / torch.sqrt(torch.tensor(6))\n",
        "attention = torch.softmax(attention, dim=-1) @ v\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmrv8Pr_9od7",
        "outputId": "a4255a21-1529-4105-857a-18a2f537a68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3528, -0.1137,  2.1152, -0.6947, -1.3591, -3.5024],\n",
              "        [-1.8490, -0.4083, -3.4174, -0.1400,  3.5685, -0.7615],\n",
              "        [-1.3947, -0.1427,  2.1633, -0.7062, -1.3753, -3.5444]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 0, 2])\n",
        "a = EmbeddingMat[a]\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W3_ikT394BJ",
        "outputId": "1d5a0b6d-6c52-4628-8d47-e515bef4c006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1445,  0.1663,  0.7507,  0.9132, -1.4355, -1.5304],\n",
              "        [ 1.6218,  1.0775,  0.4941,  0.4672, -0.3454, -1.1625],\n",
              "        [ 0.9593,  1.0600,  0.6299, -1.2867, -0.6875,  2.1382]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = a @ Keys\n",
        "q = a @ Queries\n",
        "v = a @ Values\n",
        "attention = q @ k.T / torch.sqrt(torch.tensor(6))\n",
        "attention = torch.softmax(attention, dim=-1) @ v\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DJtdCXf-Az6",
        "outputId": "4dc51def-36f0-4cf6-92b7-285c5f1a3cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.8490, -0.4083, -3.4174, -0.1400,  3.5685, -0.7615],\n",
              "        [-1.3528, -0.1137,  2.1152, -0.6947, -1.3591, -3.5024],\n",
              "        [-1.3947, -0.1427,  2.1633, -0.7062, -1.3753, -3.5444]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encodings Enter the Chat\n",
        "pos_embs = torch.randn((3, 6))"
      ],
      "metadata": {
        "id": "4Ix1fn5HCcF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a = torch.tensor([0, 1, 2])\n",
        "a = EmbeddingMat[a] # Embeddings of three random words with positional encodings\n",
        "print(a)\n",
        "a = a + pos_embs\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmtM5Z7Y_IDi",
        "outputId": "4ecc8248-b4bc-40a0-bf98-b262d53a05b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.6218,  1.0775,  0.4941,  0.4672, -0.3454, -1.1625],\n",
            "        [ 0.1445,  0.1663,  0.7507,  0.9132, -1.4355, -1.5304],\n",
            "        [ 0.9593,  1.0600,  0.6299, -1.2867, -0.6875,  2.1382]])\n",
            "tensor([[ 2.4991,  1.3619,  0.7925, -0.2592, -0.6573, -1.6185],\n",
            "        [ 0.7886,  0.7736,  1.9904,  1.6457, -0.9313, -0.6591],\n",
            "        [ 0.6852,  0.3131,  0.0466, -0.9168, -1.2431,  1.7399]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = a @ Keys\n",
        "q = a @ Queries\n",
        "v = a @ Values\n",
        "attention = q @ k.T / torch.sqrt(torch.tensor(6))\n",
        "attention = torch.softmax(attention, dim=-1) @ v\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csy3gQmX_SRZ",
        "outputId": "f51b87b9-710c-495d-c027-d4999d8a3c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9350,  1.7850, -2.0714, -1.2383,  0.5619, -1.7110],\n",
              "        [-1.8599, -0.1087, -1.2315, -0.3081,  1.6290, -2.2386],\n",
              "        [-3.7722, -1.2333, -0.4730,  0.6816,  2.2535, -2.2433]])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 0, 2])\n",
        "a = EmbeddingMat[a]\n",
        "print(a)\n",
        "a = a + pos_embs\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNPKuBcIAz8J",
        "outputId": "d61cda98-cc89-478f-e116-1e42847f1331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1445,  0.1663,  0.7507,  0.9132, -1.4355, -1.5304],\n",
            "        [ 1.6218,  1.0775,  0.4941,  0.4672, -0.3454, -1.1625],\n",
            "        [ 0.9593,  1.0600,  0.6299, -1.2867, -0.6875,  2.1382]])\n",
            "tensor([[ 1.0217,  0.4508,  1.0491,  0.1868, -1.7474, -1.9864],\n",
            "        [ 2.2659,  1.6848,  1.7337,  1.1997,  0.1588, -0.2912],\n",
            "        [ 0.6852,  0.3131,  0.0466, -0.9168, -1.2431,  1.7399]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = a @ Keys\n",
        "q = a @ Queries\n",
        "v = a @ Values\n",
        "attention = q @ k.T / torch.sqrt(torch.tensor(6))\n",
        "attention = torch.softmax(attention, dim=-1) @ v\n",
        "attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_ba6gqfA2ky",
        "outputId": "5110406b-82b5-4669-f473-471e4c3f3939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.7611, -1.2244, -0.4642,  0.6738,  2.2389, -2.2478],\n",
              "        [-1.0821,  1.1014,  1.9703, -0.9633, -1.5244, -3.1133],\n",
              "        [-3.5851, -1.0705, -0.2974,  0.5648,  1.9858, -2.3074]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can see how attention of last token is different with different orders of previous words!\n",
        "#  That's what we need ðŸ™‚\n",
        "# We don't want \"Dean is hunting ghost\" and \"Ghost is hunting dean\" to be same\n",
        "# in transformer's eyes so to speak ðŸ˜„"
      ],
      "metadata": {
        "id": "84O28AdTA-Pi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}